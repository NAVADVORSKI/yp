from airflow import DAG
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.hooks.base import BaseHook
from airflow.operators.python import PythonOperator

import datetime
import requests
import pandas as pd
import os
import psycopg2, psycopg2.extras

dag = DAG(
    dag_id='542_s3_load_example',
    schedule_interval='0 0 * * *',
    start_date=datetime.datetime(2021, 1, 1),
    catchup=False,
    dagrun_timeout=datetime.timedelta(minutes=60),
    tags=['example', 'example2'],
    params={"example_key": "example_value"},
)
business_dt = {'dt':'2022-05-06'}

def upload_from_s3(file_names):
    link = 'https://storage.yandexcloud.net/s3-sprint3-static/lessons/'
    path = '/lessons/5. Реализация ETL в Airflow/4. Extract как подключиться к хранилищу, чтобы получить файл/Задание 2/'

    for file_name in file_names:
        df = pd.read_csv(f'{link}{file_name}')
        df.to_csv(f'{path}{file_name}')

def load_file_to_pg(filename, pg_table, conn_args):

    df = pd.read_csv(f"/lessons/5. Реализация ETL в Airflow/4. Extract как подключиться к хранилищу, чтобы получить файл/Задание 2/{filename}", index_col=0)

    cols = ','.join(list(df.columns))
    insert_stmt = f"INSERT INTO stage.{pg_table} ({cols}) VALUES %s"

    pg_conn = psycopg2.connect(
        host=conn_args['host'],
        port=conn_args['port'],
        database=conn_args['database'],
        user=conn_args['user'],
        password=conn_args['password']
    )
    cur = pg_conn.cursor()

    psycopg2.extras.execute_values(cur, insert_stmt, df.values)
    pg_conn.commit()

    cur.close()
    pg_conn.close()

t_upload_from_s3 = PythonOperator(task_id='upload_from_s3',
                                        python_callable=upload_from_s3,
                                        op_kwargs={'file_names' : ['customer_research.csv'
                                                                ,'user_activity_log.csv'
                                                                ,'user_order_log.csv']
                                        },
                                        dag=dag)

conn_args={
    'host':'localhost',
    'user':'jovyan',
    'password':'jovyan',
    'database':'de',
    'port':'5432',
}

load_customer_research = PythonOperator(
    task_id='load_customer_research',
    python_callable=load_file_to_pg,
    op_kwargs={
        'filename':'customer_research.csv',
        'pg_table':'customer_research',
        'conn_args':conn_args},
    dag=dag
)

load_user_order_log = PythonOperator(
    task_id='load_user_order_log',
    python_callable=load_file_to_pg,
    op_kwargs={
        'filename':'user_order_log.csv',
        'pg_table':'user_order_log',
        'conn_args':conn_args},
    dag=dag
)

load_user_activity_log = PythonOperator(
    task_id='load_user_activity_log',
    python_callable=load_file_to_pg,
    op_kwargs={
        'filename':'user_activity_log.csv',
        'pg_table':'user_activity_log',
        'conn_args':conn_args},
        dag=dag
)

t_upload_from_s3 >> load_customer_research >> load_user_order_log >> load_user_activity_log
